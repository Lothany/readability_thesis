{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanford\n",
      "tagger acquired\n",
      "directory success\n",
      "Si Maya ay pangalawa sa tatlong magkakapatid. Nakatira sila sa abang tahanan ng Barangay San Isidro, lungsod ng Heneral Santos. Mapagmahal ang kanyang mga magulang na sina Mang Mando at Aling Memay. Mahilig maglaro si Maya sa kanyang manika, na bigay ng kanyang Ante Anna. Araw ng Sabado, naglalaro ang magkakapatid, “Kuya Miguel, maglalaro tayo ng manika ko”, tawag ni Maya sa kanyang nakatatandang kapatid. Bog,Bog,Bog “Sandali lamang naglalaro pa kami ni Marco ng basketball”, tugon naman ng kanyang Kuya Miguel. Si Marco ay ang kanilang nakababatang kapatid. Mag-isa na lamang naglaro si Maya sa kanyang manika na pinangalanan niyang Meya. “Mmmmm….mmmmm, tulog na!tulog na! Meya kong mahal”. Awit ni Maya habang inuugoy ang kanyang manika. Inilapag niya ito sa loob ng kahon bilang kama ng kanyang manika. Pumunta si Maya sa kusina upang uminom ng tubig. Nakita niya sa labas ang dalawa niyang kapatid na naglalaro ng bola. “Kuya Miguel, ipasa mo rin sa akin ang bola”, nakangiting sigaw ni Maya. “Eto, saluin mo ang bola Maya”. Tugon ni Kuya Miguel. “Wow, ang galing mo Ate Maya, nasalo mo”, papuri ni Marco sa kanyang ate. Lumabas sila sa kanilang bahay upang maglaro pa ng habol-habolan. \n",
      "Dumating ang kanyang Nanay Memay mula sa palengke. “Mano po Inay”, patakbong lumapit ang magkakapatid. “Maaari po bang maglaro pa kami sa labas, Inay?  Paalam ni Kuya Miguel kay Aling Memay. “O, sige mga anak, basta maging maingat at huwag mag-aaway,” paalala ni Aling Memay.\n",
      " “Huli ka, Marco!”  “Ako naman habulin mo Marco,” tugon ni Maya.  “Mag-iingat kayo, baka madapa kayo,” paalala ng kanilang Kuya Miguel. \n",
      "Sa kusina, inaayos ni Aling Memay ang kanyang pinamili. Meron siyang mga prutas; mangga, at mansanas.   Gulay at manok.  Nagligpit siya ng mga kalat sa loob ng sala, itinabi ang kahon at ipinasok sa bodega.\n",
      "Matapos magluto, tinawag na niya ang kanyang mga anak upang kumain ng meryenda. “Nandito na po kami Inay”, bungad ni Maya. Binigyan sila ng tinapay at juice ni Aling Memay.   “Salamat po, Inay,” nabusog po kami. Sabay sabi nina Kuya Miguel, Maya at Marco.” \n",
      "Kinabukasan. “Magandang umaga, Maya,” masayang bati ng kanyang manika. “Magandang umaga naman Meya,” tugon ni Maya sa kanyang Manika. “Halika maglaro tayo ng tagu-taguan,” sambit ni Meya Manika. “Isa, dalawa, tatlo, magtago kana.”   Hinanap ni Maya si Meya Manika, sumilip sya sa ilalim ng kama, “Huli ka!” ngunit walang Meya.  Pumunta naman siya sa kanang parte ng kanilang bahay ang sala, tahimik at wala parin si Meya. “Aaaaahhh! alam ko na,” tinungo ni Maya ang silid nila.  Dahan-dahan niyang binuksan ang pinto. “Meya, Meya nasaan ka?  Mahuhuli na kita.”  Malumanay na sambit ni Maya. Subalit wala paring Meya Manika. “Naku! Saan kaya siya nagtago? Lumabas ito ng bahay, tiningnan sa likod ng kanilang bahay, sa ilalim ng mga tanim, gayon paman wala paring Meya Manika. \n",
      "Nagsimula na siyang mag-alala. “Nasaan kana Meya? Lumabas kana?” Paiyak na sambit ni Maya.  “Inay, Itay, nawawala po si Meya.”  “Paanong nawala? Tanong ni Mang Mando. “Naglalaro lang po kami ng tagu-taguan, kanina ko pa po siya hinahanap, pero hindi ko siya mahanap.”  Malungkot na sagot ni Maya.\n",
      "“Huhuhuhu, tulungan po ninyo ako, hanapin natin si Meya.”  Iyak nang iyak si Maya.  Lumapit na rin ang kanyang Kuya Miguel at bunsong kapatid na si Marco. “Huwag ka nang umiyak Maya, tutulungan ka naming.”  Nagmungkahi ang kanyang Inay na si Aling Memay na mag imprinta sila ng larawan ni Meya Manika, ipaskil sa labas ng kanilang bahay at sa kanilang plasa. “Heto na ang larawan ni Meya, idikit natin sa labas.”   Ang wika ni Aling Memay. \n",
      "Subalit hindi pa rin nila nahanap si Memay. Iyak nang iyak parin si Maya. “Maya, Maya, gumising ka, nananaginip ka.”  “Bakit kaba umiiyak? “Inay nawawala po si Meya.”  Ang malungkot na sagot ni Maya. “Saan mo ba iniwan ang iyong manika?   Tanong naman ni Aling Nena. “Hindi ko po alam, Inay”. Dali-daling bumangon si Maya. Lumabas ng kanyang silid at hinanap si Maya. “Pumunta siya sa sala, sa kusina, sa banyo, at sa bahaging likuran ng kanilang bahay.  Subalit wala talaga si Meya Manika.  “Isipin mo nang mabuti Maya, saan ka huling naglaro ng iyong manika?   Tanong muli ni Aling Memay. “Kahapon po dito sa loob ng bahay.”  “Miguel, Marco, hali nga kayo,” tawag ni Aling Memay sa dalawang magkakapatid.  Lumapit kaagad sila at nagtanong.  “Bakit po Inay?  Nakita niyo ba ang manika ni Maya?  “Hindi po,” magalang na sagot ni Kuya Miguel.  “Ikaw Marco?  Tanong naman ni Aling Memay kay Marco. “Hindi rin po, Inay.”  “Isipin mo nang mabuti Maya, saan mo inilagay ang iyong manika? “Kahapon po pinapatulog ko po siya, ah, naaalala ko na po pinasok ko siya sa isang karton na ginawa kong kama niya.” “Dito ko po sa sahig nilapag kahapon.”  Maligsing tugon ni Maya sa kanyang Inay.  \n",
      "Dali-daling tumalikod si Aling Memay, at nagtungo sa bodega. “Ito ba na kahon Maya?   Tanong ni Maya.   “Upo , Inay yan nga po.”  Sabik na sagot ni Maya. “Heto, buksan mo,” wika ng kanyang inay. Nang buksan ni Maya, nagningning ang kanyang mga mata nang makita sa loob ng kahon si Meya Manika. “Maraming-maraming salamat po Inay.”  Niyakap nang mahigpit ni Maya si Meya Manika. Masayang-masaya din ang kanyang mga kapatid. \n",
      "Pinaalalahanan ni Aling Memay ang kanyang mga anak na, “Kapag kayo ay maglalaro, siguraduhing iligpit sa tamang lalagyan ang inyong mga laruan. Pahalagahan ninyo ang ano mang bagay na nasa inyo,” wika ng kanilang Inay.  “Opo Inay, patawad po, nakalimutan kong iligpit ang aking manika,” malumanay na sagot ni Maya.  “Opo Inay!” sagot din ni Kuya Miguel at Marco. \n",
      "Hinalikan at niyaakap muli Maya si Meya Manika. \n",
      "\n",
      "Loading default properties from tagger stanford-postagger-full-2020-11-17/models/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger\n",
      "Loading POS tagger from stanford-postagger-full-2020-11-17/models/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger ... done [0.6 sec].\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:87)\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:37)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:341)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:328)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:151)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:1044)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagCoreLabelsOrHasWords(MaxentTagger.java:1835)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagAndOutputSentence(MaxentTagger.java:1845)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1756)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1817)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1590)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1546)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1889)\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', 'stanford-postagger-full-2020-11-17/stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'stanford-postagger-full-2020-11-17/models/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger', '-textFile', '/var/folders/1x/zxvdt5995hx_mv_d5rtx8b4h0000gn/T/tmpc7b2bciq', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf-8']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_text)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# tok_text = word_tokenize(raw_text)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# fw1 = open(tokenized_data_path + \"tok_\" + filename, \"w\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# fw1.write(str(tok_text))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# fw.write(str(pos_tagger.tag(raw_text)))\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# fw.close()\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m tagged_text \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Still needs splitting, but no tokenization\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Save the tagged output in a new file\u001b[39;00m\n\u001b[1;32m     43\u001b[0m tagged_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagged_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tag/stanford.py:90\u001b[0m, in \u001b[0;36mStanfordTagger.tag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# This function should return list of tuple rather than list of list\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag_sents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, [])\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tag/stanford.py:112\u001b[0m, in \u001b[0;36mStanfordTagger.tag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    109\u001b[0m _input_fh\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Run the tagger and get the output\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m stanpos_output, _stderr \u001b[38;5;241m=\u001b[39m \u001b[43mjava\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasspath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stanford_jar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m stanpos_output \u001b[38;5;241m=\u001b[39m stanpos_output\u001b[38;5;241m.\u001b[39mdecode(encoding)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Delete the temporary file\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/internals.py:148\u001b[0m, in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_decode_stdoutdata(stderr))\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava command failed : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(cmd))\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (stdout, stderr)\n",
      "\u001b[0;31mOSError\u001b[0m: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', 'stanford-postagger-full-2020-11-17/stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', 'stanford-postagger-full-2020-11-17/models/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger', '-textFile', '/var/folders/1x/zxvdt5995hx_mv_d5rtx8b4h0000gn/T/tmpc7b2bciq', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf-8']"
     ]
    }
   ],
   "source": [
    "# Stanford POS tagger local installation to tag a directory of plain text files\n",
    "import nltk\n",
    "from nltk import *\n",
    "import os\n",
    " \n",
    "# environment variables for the Stanford PoS Tagger\n",
    "java_path = \"/Library/Java/JavaVirtualMachines/jdk-15.0.1.jdk/Contents/Home\"\n",
    "os.environ[\"JAVAHOME\"] = java_path\n",
    "print(\"stanford\")\n",
    " \n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "#from nltk.tokenize import word_tokenize\n",
    " \n",
    "model = \"stanford-postagger-full-2020-11-17/models/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger\"\n",
    "jar = \"stanford-postagger-full-2020-11-17/stanford-postagger.jar\"\n",
    "\n",
    "print(\"tagger acquired\")\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")\n",
    " \n",
    "# data path of the input corpus files as well as separate output directories for tokenized and tagged data\n",
    "data_path = \"txt_utf/gtest\"\n",
    "tokenized_data_path = \"txt_tokenized\"\n",
    "tagged_data_path = \"txt_tagged\"\n",
    "print(\"directory success\")\n",
    "\n",
    "# apply tokenization and pos-tagging to all the txt-files in the directory 'data' that follow a specific naming scheme\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        fr = open(os.path.join(data_path, filename), encoding = \"utf-8\")\n",
    "        raw_text = fr.read() \n",
    "        print(raw_text)\n",
    "        tagged_text = pos_tagger.tag(raw_text.split())  # Still needs splitting, but no tokenization\n",
    "\n",
    "        # Save the tagged output in a new file\n",
    "        tagged_filename = \"tagged_\" + filename\n",
    "        with open(os.path.join(data_path, tagged_filename), \"w\", encoding=\"utf-8\") as fw:\n",
    "            fw.write(str(tagged_text))  # Save the POS-tagged text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanford\n",
      "tagger acquired\n",
      "directory success\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/gilbert/nltk_data'\n    - '/Users/gilbert/Desktop/School Stuff/Year 4/thesis/code/this_env/nltk_data'\n    - '/Users/gilbert/Desktop/School Stuff/Year 4/thesis/code/this_env/share/nltk_data'\n    - '/Users/gilbert/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m fr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, filename), encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m raw_text \u001b[38;5;241m=\u001b[39m fr\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 31\u001b[0m tok_text \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m fw1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(tokenized_data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtok_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m fw1\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(tok_text))\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/gilbert/nltk_data'\n    - '/Users/gilbert/Desktop/School Stuff/Year 4/thesis/code/this_env/nltk_data'\n    - '/Users/gilbert/Desktop/School Stuff/Year 4/thesis/code/this_env/share/nltk_data'\n    - '/Users/gilbert/Desktop/School Stuff/Year 4/thesis/code/this_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Stanford POS tagger local installation to tag a directory of plain text files\n",
    "import nltk\n",
    "from nltk import *\n",
    "import os\n",
    " \n",
    "# environment variables for the Stanford PoS Tagger\n",
    "java_path = \"/Library/Java/JavaVirtualMachines/jdk-15.0.1.jdk/Contents/Home\"\n",
    "os.environ[\"JAVAHOME\"] = java_path\n",
    "print(\"stanford\")\n",
    " \n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "model = \"stanford-postagger-full-2020-11-17/models/filipino-left5words-owlqn2-distsim-pref6-inf2.tagger\"\n",
    "jar = \"stanford-postagger-full-2020-11-17/stanford-postagger.jar\"\n",
    "\n",
    "print(\"tagger acquired\")\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding = \"utf-8\")\n",
    " \n",
    "# data path of the input corpus files as well as separate output directories for tokenized and tagged data\n",
    "data_path = \"txt_utf/gtest\"\n",
    "tokenized_data_path = \"txt_tokenized\"\n",
    "tagged_data_path = \"txt_tagged\"\n",
    "print(\"directory success\")\n",
    "\n",
    "# apply tokenization and pos-tagging to all the txt-files in the directory 'data' that follow a specific naming scheme\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        fr = open(os.path.join(data_path, filename), encoding = \"utf-8\")\n",
    "        raw_text = fr.read()\n",
    "        tok_text = word_tokenize(raw_text)\n",
    "        fw1 = open(tokenized_data_path + \"tok_\" + filename, \"w\")\n",
    "        fw1.write(str(tok_text))\n",
    "        fw1.close()\n",
    "        fw = open(tagged_data_path + \"tag_\" + filename, \"w\")\n",
    "        fw.write(str(pos_tagger.tag(tok_text)))\n",
    "        fw.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "this_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
